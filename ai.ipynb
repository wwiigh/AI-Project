{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOUTcxSEI52qwC66U0Y+knJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wwiigh/AI-Project/blob/dataset2/ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xQ67hzhaQuJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ce4442-9b6e-4a41-8da3-64515a16f4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'AI-Project' already exists and is not an empty directory.\n",
            "/content/AI-Project\n",
            "now branch\n",
            "* \u001b[32mdataset2\u001b[m\n",
            "  main\u001b[m\n",
            "\n",
            "total branch\n",
            "* \u001b[32mdataset2\u001b[m\n",
            "  main\u001b[m\n",
            "  \u001b[31mremotes/origin/HEAD\u001b[m -> origin/main\n",
            "  \u001b[31mremotes/origin/dataset1\u001b[m\n",
            "  \u001b[31mremotes/origin/dataset2\u001b[m\n",
            "  \u001b[31mremotes/origin/main\u001b[m\n",
            "  \u001b[31mremotes/origin/question1\u001b[m\n",
            "  \u001b[31mremotes/origin/question2\u001b[m\n",
            "\n",
            "change from main to dataset2\n",
            "D\tcached_lm_GPT2Tokenizer_128_train_dataset.txt\n",
            "D\tcached_lm_GPT2Tokenizer_128_valid_dataset.txt\n",
            "M\tprocess.py\n",
            "M\tprocessed.csv\n",
            "M\ttrain_dataset.txt\n",
            "M\tunprocessed.csv\n",
            "M\tvalid_dataset.txt\n",
            "Already on 'dataset2'\n",
            "Your branch is up to date with 'origin/dataset2'.\n",
            "* \u001b[32mdataset2\u001b[m\n",
            "  main\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/wwiigh/AI-Project.git\n",
        "%cd /content/AI-Project\n",
        "print(\"now branch\")\n",
        "!git branch\n",
        "print(\"\\ntotal branch\")\n",
        "!git branch -a\n",
        "print(\"\\nchange from main to dataset2\")\n",
        "!git checkout dataset2\n",
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python process.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GlzEN-3T9BF",
        "outputId": "41d9245f-0837-4450-b9f3-af7c797dc588"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    Use:\\n1 c. firmly packed brown sugar\\n1/2 c. e...\n",
            "1    Use:\\n1 small jar chipped beef, cut up\\n4 bone...\n",
            "2    Use:\\n2 (16 oz.) pkg. frozen corn\\n1 (8 oz.) p...\n",
            "3    Use:\\n1 large whole chicken\\n2 (10 1/2 oz.) ca...\n",
            "4    Use:\\n1 c. peanut butter\\n3/4 c. graham cracke...\n",
            "Name: full_recipe, dtype: object\n",
            "Use:\n",
            "1 c. firmly packed brown sugar\n",
            "1/2 c. evaporated milk\n",
            "1/2 tsp. vanilla\n",
            "1/2 c. broken nuts (pecans)\n",
            "2 tbsp. butter or margarine\n",
            "3 1/2 c. bite size shredded rice biscuits\n",
            "\n",
            "Title:\n",
            "No-Bake Nut Cookies\n",
            "\n",
            "Ingredients:\n",
            "1 c. firmly packed brown sugar\n",
            "1/2 c. evaporated milk\n",
            "1/2 tsp. vanilla\n",
            "1/2 c. broken nuts (pecans)\n",
            "2 tbsp. butter or margarine\n",
            "3 1/2 c. bite size shredded rice biscuits\n",
            "\n",
            "Directions:\n",
            "in a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\n",
            "stir over medium heat until mixture bubbles all over top.\n",
            "boil and stir 5 minutes more. take off heat.\n",
            "stir in vanilla and cereal; mix well.\n",
            "using 2 teaspoons, drop and shape into 30 clusters on wax paper.\n",
            "let stand until firm, about 30 minutes.\n",
            "0    Use:\\nfirmly packed brown sugar\\nevaporated mi...\n",
            "1    Use:\\nsmall chipped beef, cut up\\nboned chicke...\n",
            "2    Use:\\nfrozen corn\\ncream cheese, cubed\\nbutter...\n",
            "3    Use:\\nlarge whole chicken\\nchicken gravy\\ncrea...\n",
            "4    Use:\\npeanut butter\\ngraham cracker crumbs\\nme...\n",
            "Name: full_recipe, dtype: object\n",
            "Use:\n",
            "firmly packed brown sugar\n",
            "evaporated milk\n",
            "vanilla\n",
            "broken nuts (pecans)\n",
            "butter or margarine\n",
            "bite size shredded rice biscuits\n",
            "\n",
            "Title:\n",
            "No-Bake Nut Cookies\n",
            "\n",
            "Ingredients:\n",
            "1 c. firmly packed brown sugar\n",
            "1/2 c. evaporated milk\n",
            "1/2 tsp. vanilla\n",
            "1/2 c. broken nuts (pecans)\n",
            "2 tbsp. butter or margarine\n",
            "3 1/2 c. bite size shredded rice biscuits\n",
            "\n",
            "Directions:\n",
            "in a heavy 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\n",
            "stir over medium heat until mixture bubbles all over top.\n",
            "boil and stir 5 minutes more. take off heat.\n",
            "stir in vanilla and cereal; mix well.\n",
            "using 2 teaspoons, drop and shape into 30 clusters on wax paper.\n",
            "let stand until firm, about 30 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9-nUxKAUZuc",
        "outputId": "d86edb3c-eb35-403b-97a7-d4c5b9cc8f2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "ND2qDG8OUCzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfc80d7-39f6-41a4-9199-1e6fb3b1c9a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-07 13:58:10.703485: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-07 13:58:10.703538: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-07 13:58:10.705069: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-07 13:58:11.973045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n",
            " 11% 249/2170 [00:17<02:06, 15.22it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 35% 6/17 [00:00<00:00, 49.35it/s]\u001b[A\n",
            " 65% 11/17 [00:00<00:00, 40.50it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8911501169204712, 'eval_runtime': 0.4765, 'eval_samples_per_second': 281.235, 'eval_steps_per_second': 35.679, 'epoch': 0.81}\n",
            " 12% 250/2170 [00:17<02:06, 15.22it/s]\n",
            "100% 17/17 [00:00<00:00, 38.37it/s]\u001b[A\n",
            "{'loss': 2.0603, 'grad_norm': 6.507040023803711, 'learning_rate': 3.847926267281106e-05, 'epoch': 1.61}\n",
            " 23% 500/2170 [00:33<01:47, 15.56it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 44.31it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 38.53it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.8149133920669556, 'eval_runtime': 0.4747, 'eval_samples_per_second': 282.284, 'eval_steps_per_second': 35.812, 'epoch': 1.61}\n",
            " 23% 500/2170 [00:34<01:47, 15.56it/s]\n",
            "100% 17/17 [00:00<00:00, 37.46it/s]\u001b[A\n",
            " 35% 749/2170 [00:50<01:29, 15.95it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 44.74it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 38.91it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 1.784498691558838, 'eval_runtime': 0.4798, 'eval_samples_per_second': 279.265, 'eval_steps_per_second': 35.429, 'epoch': 2.42}\n",
            " 35% 750/2170 [00:50<01:29, 15.95it/s]\n",
            "100% 17/17 [00:00<00:00, 37.32it/s]\u001b[A\n",
            "{'loss': 1.7403, 'grad_norm': 5.772319793701172, 'learning_rate': 2.6958525345622122e-05, 'epoch': 3.23}\n",
            " 46% 1000/2170 [01:07<01:14, 15.75it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 44.42it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 37.95it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7827239036560059, 'eval_runtime': 0.4735, 'eval_samples_per_second': 282.997, 'eval_steps_per_second': 35.903, 'epoch': 3.23}\n",
            " 46% 1000/2170 [01:07<01:14, 15.75it/s]\n",
            "100% 17/17 [00:00<00:00, 37.29it/s]\u001b[A\n",
            " 58% 1249/2170 [01:35<00:58, 15.86it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 42.75it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 38.79it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7707685232162476, 'eval_runtime': 0.4789, 'eval_samples_per_second': 279.802, 'eval_steps_per_second': 35.497, 'epoch': 4.03}\n",
            " 58% 1250/2170 [01:35<00:58, 15.86it/s]\n",
            "100% 17/17 [00:00<00:00, 37.61it/s]\u001b[A\n",
            "{'loss': 1.5995, 'grad_norm': 5.465277194976807, 'learning_rate': 1.543778801843318e-05, 'epoch': 4.84}\n",
            " 69% 1500/2170 [01:51<00:42, 15.89it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 44.75it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 39.24it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7632105350494385, 'eval_runtime': 0.4697, 'eval_samples_per_second': 285.262, 'eval_steps_per_second': 36.19, 'epoch': 4.84}\n",
            " 69% 1500/2170 [01:52<00:42, 15.89it/s]\n",
            "100% 17/17 [00:00<00:00, 37.07it/s]\u001b[A\n",
            " 81% 1749/2170 [02:08<00:27, 15.51it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 45.55it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 39.13it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7660574913024902, 'eval_runtime': 0.4749, 'eval_samples_per_second': 282.141, 'eval_steps_per_second': 35.794, 'epoch': 5.65}\n",
            " 81% 1750/2170 [02:09<00:27, 15.51it/s]\n",
            "100% 17/17 [00:00<00:00, 38.14it/s]\u001b[A\n",
            "{'loss': 1.5115, 'grad_norm': 5.47285795211792, 'learning_rate': 3.9170506912442395e-06, 'epoch': 6.45}\n",
            " 92% 2000/2170 [02:25<00:10, 15.66it/s]\n",
            "  0% 0/17 [00:00<?, ?it/s]\u001b[A\n",
            " 29% 5/17 [00:00<00:00, 45.11it/s]\u001b[A\n",
            " 59% 10/17 [00:00<00:00, 39.05it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.7673836946487427, 'eval_runtime': 0.4725, 'eval_samples_per_second': 283.626, 'eval_steps_per_second': 35.982, 'epoch': 6.45}\n",
            " 92% 2000/2170 [02:25<00:10, 15.66it/s]\n",
            "100% 17/17 [00:00<00:00, 37.30it/s]\u001b[A\n",
            "100% 2169/2170 [02:43<00:00, 14.69it/s]There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "{'train_runtime': 163.3078, 'train_samples_per_second': 53.065, 'train_steps_per_second': 13.288, 'train_loss': 1.70814699797037, 'epoch': 7.0}\n",
            "100% 2170/2170 [02:43<00:00, 13.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOZnjrzTaOyk",
        "outputId": "127fafed-dcca-415d-8275-a802b4eb4ddf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-05-07 14:02:59.209978: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-05-07 14:02:59.210038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-05-07 14:02:59.212122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-05-07 14:03:01.043671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Use:\n",
            "chocolate\n",
            "flour\n",
            "milk\n",
            "eggs\n",
            "\n",
            "Title:\n",
            "chocolate cake\n",
            "\n",
            "Ingredients:\n",
            "1/2 c. chocolate\n",
            "1 stick margarine\n",
            "2 c. packed brown sugar\n",
            "1 c. milk\n",
            "3 eggs\n",
            "\n",
            "Directions:\n",
            "cream the chocolate.\n",
            "mix well.\n",
            "beat until very stiff. add milk slowly:\n",
            "beat at low speed until smooth. add eggs and beat.\n",
            "fold in chocolate pudding. stir well.\n",
            "fold firmly in brown sugar mixture.\n",
            "spread over cake in a platter of lightly greased and floured pans.\n",
            "makes about 2 dozen.\n",
            "Use:\n",
            "marshmallows\n",
            "melted margarine\n",
            "chocolate\n",
            "milk\n",
            "eggs\n",
            "\n",
            "Title:\n",
            "Strawberry Pie\n",
            "\n",
            "Ingredients:\n",
            "1 pkg. marshmallows\n",
            "1 c. melted margarine\n",
            "1 c. chocolate\n",
            "1 (7 oz.) pkg. butter\n",
            "1 c. chocolate\n"
          ]
        }
      ]
    }
  ]
}